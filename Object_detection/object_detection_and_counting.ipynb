{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2226d9-e361-4d9e-b50d-2cf8c848c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: filelock in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: numpy in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (4.10.0)\n",
      "Requirement already satisfied: matplotlib in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/disha_himani/anaconda3/envs/flipkartgrid1/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b9f075-bb7e-4ba1-afc4-dc628a91e036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 7 objects.\n",
      "Object counts: {50: 1, 55: 3, 67: 1, 84: 1, 86: 1}\n",
      "Final Object Counts:\n",
      "Class 50: 1 objects detected\n",
      "Class 55: 3 objects detected\n",
      "Class 67: 1 objects detected\n",
      "Class 84: 1 objects detected\n",
      "Class 86: 1 objects detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the Faster R-CNN model pre-trained on COCO dataset\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Transform function for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Function to detect and count objects\n",
    "def detect_and_count_objects(img_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Perform detection\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "\n",
    "    # Process results\n",
    "    threshold = 0.5\n",
    "    boxes = outputs[0]['boxes'][outputs[0]['scores'] > threshold].numpy()\n",
    "    labels = outputs[0]['labels'][outputs[0]['scores'] > threshold].numpy()\n",
    "\n",
    "    counts = {}\n",
    "    for label in np.unique(labels):\n",
    "        counts[label] = np.sum(labels == label)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Detected {len(boxes)} objects.\")\n",
    "    print(\"Object counts:\", counts)\n",
    "\n",
    "    # Visualize the detected boxes\n",
    "    img = cv2.imread(img_path)\n",
    "    for box, label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(img, str(label), (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    # Convert BGR to RGB for display\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Test the function with an image\n",
    "test_file_path = 'object_detection_test.jpg'  # Replace with your image path\n",
    "object_counts = detect_and_count_objects(test_file_path)\n",
    "\n",
    "# Print final counts\n",
    "print(\"Final Object Counts:\")\n",
    "for label, count in object_counts.items():\n",
    "    print(f\"Class {label}: {count} objects detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae17210c-d18f-41f5-953c-03429fbec785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects detected: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision.ops import nms\n",
    "\n",
    "# Load the Faster R-CNN model pre-trained on COCO dataset\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define transformation to convert image to tensor\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image_tensor = transform(image)  # Apply transformation (convert to tensor)\n",
    "    return image, image_tensor\n",
    "\n",
    "# Example image (replace this with your image path)\n",
    "image_path = \"object_detection_test.jpg\"\n",
    "image, image_tensor = load_image(image_path)\n",
    "\n",
    "# Run the image through the model\n",
    "with torch.no_grad():\n",
    "    # Send image tensor as a batch (even if it's a single image)\n",
    "    predictions = model([image_tensor])\n",
    "\n",
    "# Extract predictions (bounding boxes, labels, and scores)\n",
    "predicted_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "predicted_labels = predictions[0]['labels'].cpu().numpy()\n",
    "predicted_scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Filter predictions by a higher confidence threshold\n",
    "confidence_threshold = 0.7  # Adjust threshold based on your dataset\n",
    "high_confidence_boxes = predicted_boxes[predicted_scores >= confidence_threshold]\n",
    "high_confidence_labels = predicted_labels[predicted_scores >= confidence_threshold]\n",
    "high_confidence_scores = predicted_scores[predicted_scores >= confidence_threshold]\n",
    "\n",
    "# Apply Non-Maximum Suppression (NMS)\n",
    "keep = nms(torch.tensor(high_confidence_boxes), torch.tensor(high_confidence_scores), iou_threshold=0.5)\n",
    "\n",
    "# Only keep the boxes, labels, and scores after NMS\n",
    "nms_boxes = high_confidence_boxes[keep]\n",
    "nms_labels = high_confidence_labels[keep]\n",
    "nms_scores = high_confidence_scores[keep]\n",
    "\n",
    "# Count the number of objects detected after NMS\n",
    "num_objects_detected = len(nms_boxes)\n",
    "print(f\"Number of objects detected: {num_objects_detected}\")\n",
    "\n",
    "# Define a function to draw bounding boxes on the image\n",
    "def draw_boxes(image, boxes):\n",
    "    image_with_boxes = np.array(image.copy())\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Red box\n",
    "    return Image.fromarray(image_with_boxes)\n",
    "\n",
    "# Draw the bounding boxes on the original image\n",
    "image_with_boxes = draw_boxes(image, nms_boxes)\n",
    "\n",
    "# Display the image with bounding boxes\n",
    "plt.imshow(image_with_boxes)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13a4ba18-973c-43c1-a853-b71992ef4528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no splitting\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pre-trained ResNet model\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
    "resnet.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Remove the last classification layer (we only use it for feature extraction)\n",
    "model = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features from an image\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Apply transformations\n",
    "    with torch.no_grad():\n",
    "        features = model(image)  # Extract features\n",
    "    return features.cpu().numpy().flatten()  # Flatten the features\n",
    "\n",
    "# Paths and dataset directories\n",
    "data_dir = \"fmcg_products\"  # Replace with your dataset folder path\n",
    "items = os.listdir(data_dir)\n",
    "\n",
    "# Lists to hold features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate over the dataset and extract features for all items\n",
    "for item_name in items:\n",
    "    item_dir = os.path.join(data_dir, item_name)\n",
    "    for image_name in os.listdir(item_dir):\n",
    "        image_path = os.path.join(item_dir, image_name)\n",
    "        \n",
    "        # Skip directories or hidden files\n",
    "        if os.path.isdir(image_path) or image_name.startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        # Extract features for each image\n",
    "        features = extract_features(image_path)\n",
    "        X.append(features)\n",
    "        y.append(item_name)\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train an SVM classifier on the entire dataset\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X, y_encoded)\n",
    "\n",
    "# Function to classify a new image\n",
    "def classify_image(image_path):\n",
    "    features = extract_features(image_path)\n",
    "    prediction = clf.predict([features])\n",
    "    predicted_label = label_encoder.inverse_transform(prediction)[0]\n",
    "    return predicted_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287230c2-7a1a-486f-bafa-d4d173eb6ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flipkartgrid",
   "language": "python",
   "name": "flipkartgrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
